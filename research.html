<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Rishabh Agrawal</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="owwwlab.com">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        
        <meta name="description" content="Rishabh Agrawal USC IIT Guwahati UMN Hanyang Samsung" />
        <meta name="keywords" content="Rishabh Agrawal, USC, IIT Guwahati, Samsung, UMN, Hanyang" />

        <link rel="shortcut icon" href="./images/ra-icon.png">

        <!--CSS styles-->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">  
        <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
        <link rel="stylesheet" href="css/magnific-popup.css">
        <link rel="stylesheet" href="css/style.css">
        <link id="theme-style" rel="stylesheet" href="css/styles/blue.css">

        
        <!--/CSS styles-->
        <!--Javascript files-->
        <script type="text/javascript" src="js/jquery-1.11.3.min.js"></script>
        <script type="text/javascript" src="js/TweenMax.min.js"></script>
        <script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
        <script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>
        
        <script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
        <script type="text/javascript" src="js/jquery.dropdownit.js"></script>

        <script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

        <script type="text/javascript" src="js/bootstrap.min.js"></script>

        <script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

        <script type="text/javascript" src="js/masonry.min.js"></script>

        <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
        <script type="text/javascript" src="js/jquery.nicescroll.min.js"></script>

        <script type="text/javascript" src="js/magnific-popup.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>

        <!--/Javascript files-->

    </head>
    <body>

        <div id="wrapper">
            <a href="#sidebar" class="mobilemenu"><i class="fa fa-reorder"></i></a>

            <div id="sidebar">
                <div id="sidebar-wrapper">
                    <div id="sidebar-inner">
                        <!-- Profile/logo section-->
                        <div id="profile" class="clearfix">
                            <a href="index.html">
                                <div class="portrait hidden-xs"></div>
                                <div class="title">
                                    <h3>Rishabh Agrawal</h3>
                                    <!-- <h3>University of Southern California</h3> -->
                            </a>
                            <a title="USC" href="https://www.usc.edu/" target="_blank">
                                <h5 style="color: #aaa">University of Southern California,</h5> 
                                <h5 style="margin-top: -3%; color: #aaa;"> Los Angeles</h5>
                            </a>
                        </div>   
                    </div>
                        <!-- /Profile/logo section-->

                        <!-- Main navigation-->
                        <div id="main-nav">
                            <ul id="navigation">
                                <li>
                                  <a href="index.html">
                                    <i class="fa fa-user"></i>
                                    <div class="text">About</div>
                                  </a>
                                </li>  
                                
                                <li class="currentmenu">
                                  <a href="research.html">
                                    <i class="fa fa-edit"></i>
                                    <div class="text">Research</div>
                                  </a>
                                </li> 


                                 <!-- <li>
                                  <a href="life.html">
                                    <i class='fa fa-heartbeat'></i>
                                      <div class="text">Life</div>
                                  </a>
                                </li>


                                <li>
                                  <a href="contact.html">
                                      <i class="fa fa-map-marker"></i>
                                      <div class="text">Find Me</div>
                                  </a>
                                </li> -->

                            </ul>
                        </div>


                        <!-- Social media -->

                        <div id="rcorners">
                            <div class="grid-container">
                                <item><a title="Email" href="mailto:rishabha@usc.edu"><i class="fa fa-envelope"></i></a></item>
                                <item><a title="GitHub" href="https://github.com/rishabh-1086" target="_blank"><i class="fa fa-github""></i></a></item>
                                <item><a title="Google Scholar" href="https://scholar.google.com/citations?user=LSIA40MAAAAJ&hl=en" target="_blank"><i class="fa fa-graduation-cap" style="transform: scale(-1, 1);"></i></a></item>
                                <item><a title="LinkedIn" href="https://www.linkedin.com/in/rishabh-694/" target="_blank"><i class="fa fa-linkedin"></i></a></item>
                                <item><a title="Facebook" href="https://www.facebook.com/rishabh.agrawal.7/" target="_blank"><i class="fa fa-facebook-official"></i></a></item>
                                <item><a title="Instagram" href=https://www.instagram.com/__rishabh.agrawal__/" target="_blank"><i class="fa fa-instagram"></i></a></item>
                                <item></item>
                                <!-- <item>
                                    <a title="Resume" href="https://drive.google.com/file/d/1wxloraTS4kUvf7WtTnpk8pEpm3jT1Eux/view?usp=sharing" target="_blank"><i class="fa fa-file"></i></a>                                  
                                </item> -->
                                <item>
                                    <a title="Resume" href="https://drive.google.com/file/d/1HXflkbFMkekkb3gF9U-okVL8nOGzdNnU/view?usp=sharing" target="_blank">
                                        <span class="fa-stack fa-1x">
                                            <i class="fa fa-file fa-stack-1x"></i>
                                              <span class="fa fa-stack-1x" style="color:rgb(0, 0, 0);">
                                                  <span style="font-size:11px; margin-top:0px; font-weight: bold;">
                                                      CV
                                                  </span>
                                            </span>
                                        </span>	
                                
                                    </a>
                                </item>
                                <item></item>                                    
                            </div>
                        </div>

                        <!-- /Main navigation-->
                        <!-- Sidebar footer -->
                        <div id="sidebar-footer">
                            <div class="lastupdate" style="padding: 5px;">Last Updated: May 2024</div>
                            <!-- <div class="social-icons">
                                <ul>
                                    <li><a href="https://github.com/agnihotriakhil"><i class="fa fa-github"></i></a></li>
                                    <li><a href="https://scholar.google.com/citations?user=Kf1o27gAAAAJ&hl=en"><i class="fa fa-google"></i></a></li>
                                    <li><a href="https://www.linkedin.com/in/akhil-agnihotri-549690137/"><i class="fa fa-linkedin"></i></a></li>
                                    <li><a href="https://www.facebook.com/akhil.agnihotri.35"><i class="fa fa-facebook"></i></a></li>
                                    <li><a href="https://www.instagram.com/longstory_short_/"><i class="fa fa-instagram"></i></a></li>
                                </ul>
                            </div> -->
                            <!-- <div id="copyright">
                                <i class="fa fa-copyright">Akhil Agnihotri</i>
                            </div> -->
                    
                        </div>
                        <!-- /Sidebar footer -->
                    </div>

                </div>
            </div>

            <div id="main">
            
                <div id="research" class="page">
                    <div class="pageheader">

                        <div class="headercontent">

                            <div class="section-container">
                                <h3 class="title" > Research Summary </h3>
                            
                                <div class="row" style="margin: 0px 0px 0px 10px;">
                                    <div class="col-md">
                                    <font size="-0.5">
                                        <p>
                                            I am passionate about addressing problems that have a direct and meaningful impact on our community. My work has been dedicated to understanding the nature of these challenges and exploring solutions rooted in machine learning, probability, optimization, and simulation.
                                        </p>
                                        <p> 
                                            <strong>Interests</strong>: Reinforcement Learning, Imitation Learning, Generative AI, Behavior Foundation Models, Large Language Models.
                                        </p>
                                    </font>
                                    </div>
                                    <!-- <div class="col-md-4">
                                        <div class="subtitle text-center">
                                            <h3>Interests</h3>
                                        </div>
                                        <ul class="ul-boxed list-unstyled">
                                            <li>Data Mining</li>
                                            <li>Machine Learning</li>
                                            <li>Deep Learning</li>
                                            <li>Recommendation Systems</li>
                                            <li>User Online Experience</li>
                                            <li> Social Media </li>
                                            <li> Big Data </li>
                                        </ul>
                                    </div> -->
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="pagecontents">
                        
                        <div class="section color-1">
                            <div class="section-container">
                                
                                <div class="row">

                                    <!-- Publications -->
                                    <div class="col-md-10 col-md-offset-1" >
                                        <div class="title text-center">
                                            <h3 style="font-weight: 400;">Publications</h3>
                                        </div>
                                        <ul class="ul-withdetails">


                                            <!-- Markov Balance Satisfaction Improves Performance in Strictly Batch Offline Imitation Learning -->
                                            <li>
                                                <div class="row">
                                                    <!-- <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/preference_bandits_teaser.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div> -->
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="video thumb">
                                                            <video class="responsive-video" controls>
                                                                <source src="videos/HalfCheetah_video.mp4" type="video/mp4">
                                                                Your browser does not support the video tag.
                                                            </video>
                                                            <div class="videooverlay">
                                                                <i class="fa fa-play-circle"></i>
                                                            </div>
                                                        </div>
                                                    </div>


                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 32px;">Markov Balance Satisfaction Improves Performance in Strictly Batch Offline Imitation Learning                                                       
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar. </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                AAAI 2025 &nbsp;
                                                                [<a href="https://arxiv.org/pdf/2408.09125" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Imitation learning (IL) is notably effective for robotic tasks where directly programming behaviors or defining optimal control costs is challenging. In this work, we address a scenario where the imitator relies solely on observed behavior and cannot make environmental interactions during learning. It does not have additional supplementary datasets beyond the expert’s dataset nor any information about the transition dynamics. Unlike state-of-the-art (SOTA) IL methods, this approach tackles the limitations of conventional IL by operating in a more constrained and realistic setting. Our method uses the Markov balance equation and introduces a novel conditional density estimation-based imitation learning framework. It employs conditional normalizing flows for transition dynamics estimation and aims at satisfying a balance equation for the environment. Through a series of numerical experiments on Classic Control and MuJoCo environments, we demonstrate consistently superior empirical performance compared to many SOTA IL algorithms.
                                                    </p>
                                                    
                                                </div>
                                            </li>


                                            
                                            <!-- Conditional Kernel Imitation Learning for Continuous State Environments. -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="video thumb">
                                                            <video class="responsive-video" controls>
                                                                <source src="videos/LunarLander.mp4" type="video/mp4">
                                                                Your browser does not support the video tag.
                                                            </video>
                                                            <div class="videooverlay">
                                                                <i class="fa fa-play-circle"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 32px;">Conditional Kernel Imitation Learning for Continuous State Environments.                                                       
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar. </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                Submitted L4DC 2025 &nbsp;
                                                                [<a href="https://arxiv.org/pdf/2308.12573" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Imitation Learning (IL) is an important paradigm within
the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of rewardfeedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods
such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is
particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems
which often require additional online interaction data to be
effective. In this paper, we consider the problem of imitation
learning in continuous state space environments based solely
on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any
additional interactions with the environment. Our approach
is based on the Markov balance equation and introduces a
novel conditional kernel density estimation-based imitation
learning framework. It involves estimating the environment’s
transition dynamics using conditional kernel density estimators and seeks to satisfy the probabilistic balance equations
for the environment. We establish that our estimators satisfy
basic asymptotic consistency requirements. Through a series
of numerical experiments on continuous state benchmark environments, we show consistently superior empirical performance over many state-of-the-art IL algorithms.
                                                    </p>
                                                    
                                                </div>
                                            </li>




                                            <!-- Policy Optimization for Strictly Batch Imitation Learning -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="video thumb">
                                                            <video class="responsive-video" controls>
                                                                <source src="videos/CartPole.mp4" type="video/mp4">
                                                                Your browser does not support the video tag.
                                                            </video>
                                                            <div class="videooverlay">
                                                                <i class="fa fa-play-circle"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 32px;">Policy Optimization for Strictly Batch Imitation Learning                                                      
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar. </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                OPT for ML, NeurIPS 2024 &nbsp;
                                                                [<a href="https://opt-ml.org/papers/2024/paper84.pdf" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Imitation Learning (IL) offers a compelling framework within the broader context of Reinforcement Learning (RL) by eliminating the need for explicit reward feedback, a common requirement
in RL. In this work, we address IL based solely on observed behavior without access to transition
dynamics information, reward structure, or, most importantly, any additional interactions with the
environment. Our approach leverages conditional kernel density estimation and performs policy
optimization to ensure the satisfaction of the Markov balance equation associated with the environment. This method performs effectively in discrete and continuous state environments, providing
a novel solution to IL problems under strictly offline optimization settings. We establish that our
estimators satisfy basic asymptotic consistency requirements. Through a series of numerical experiments on continuous state benchmark environments, we show consistently superior empirical
performance over many state-of-the-art IL algorithms.
                                                    </p>
                                                    
                                                </div>
                                            </li>


                                            <!-- A Reinforcement Learning Framework for QoS-Driven Radio Resource Scheduler -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/RLScheduler.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 33px;">A Reinforcement Learning Framework for QoS-Driven Radio Resource Scheduler                                                     
                                                            </h3>
                                                            <p> Jitender Singh Shekhawat, <strong style="font-weight: 600;">Rishabh Agrawal</strong>, K Gautam Shenoy, Rajath Shashidhara.</p>
                                                            <p style="font-size:14.5px; margin-top: -5px;" >
                                                                Globecom 2020 &nbsp;
                                                                [<a href="https://ieeexplore.ieee.org/document/9322182" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: In cellular communication systems, radio resources are allocated to users by the MAC scheduler, that typically runs at the base station (BS). The task of the scheduler is to meet the quality of service (QoS) requirements of each data flow while maximizing the system throughput and achieving a desired level of fairness amongst users. Traditional schedulers use handcrafted metrics and are meticulously tuned to achieve a delicate balance between multiple, often conflicting objectives. Diverse QoS requirements of 5G networks further complicate traditional schedulers. In this paper, we propose a novel reinforcement learning based scheduler that learns an allocation policy to simultaneously optimize multiple objectives. Our approach allows network operators to customize their requirements, by assigning priority values to QoS classes. In addition, we adopt a flexible neural-network architecture that can easily adapt to varying number of flows, drastically simplifying training, thus rendering it viable for practical implementation in constrained systems. We demonstrate, via simulations, that our algorithm outperforms conventional heuristics such as M-LWDF, EXP-RULE and LOGRULE and is robust to changes in radio environment and traffic patterns.
                                                    </p>
                                                    
                                                </div>
                                            </li>

                                            
                                            <!-- CoPASample: A Heuristics Based Covariance Preserving Data Augmentation -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/CopaSample.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>CoPASample: A Heuristics Based Covariance Preserving Data Augmentation                                                    
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Paridhi Kothari. </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                LOD 2019 &nbsp;
                                                                [<a href="https://link.springer.com/chapter/10.1007/978-3-030-37599-7_26" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: An efficient data augmentation algorithm generates samples that improves accuracy and robustness of training models. Augmentation with informative samples imparts meaning to the augmented data set. In this paper, we propose CoPASample (Covariance Preserving Algorithm for generating Samples), a data augmentation algorithm that generates samples which reflects the first and second order statistical information of the data set, thereby augmenting the data set in a manner that preserves the total covariance of the data. To address the issue of exponential computations in the generation of points for augmentation, we formulate an optimisation problem motivated by the approach used in 
-SVR to iteratively compute a heuristics based optimal set of points for augmentation in polynomial time. Experimental results for several data sets and comparisons with other data augmentation algorithms validate the potential of our proposed algorithm.
                                                    </p>
                                                    
                                                </div>
                                            </li>

                                            <!-- Determining the Optimal Fuzzifier Range for Alpha-Planes of General Type-2 Fuzzy Sets -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/GT2.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Determining the Optimal Fuzzifier Range for Alpha-Planes of General Type-2 Fuzzy Sets                                                       
                                                            </h3>
                                                            <p> Shreyas Kulkarni, <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Frank Chung-Hoon Rhee.</p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                FUZZ-IEEE 2018 &nbsp;
                                                                [<a href="https://ieeexplore.ieee.org/document/8491556" 
                                                                target="_blank">link</a>]
                        
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Type-2 fuzzy sets (T2 FSs) are capable of handling uncertainty more efficiently than type-1 fuzzy sets (T1 FSs). The fuzzifier parameter plays an important role in the final cluster partitions in fuzzy c-means (FCM), interval type-2 (IT2) FCM, general type-2 (GT2) FCM, and other fuzzy clustering algorithms. In general, fuzzifiers are chosen for a given dataset based on experience. In this paper, we adaptively compute suitable values for the range of the fuzzifier parameter for each α-plane of GT2 FSs for a given data set. The footprint of uncertainty (FOU) for each α-plane is obtained from the given data set using histogram based membership generation. This is iteratively processed to give the converged values of fuzzifier parameters for each α-plane of GT2 FSs. Experimental results for several data sets are given to validate the effectiveness of our proposed method.
                                                    </p>
                                                    
                                                </div>
                                            </li>

                                             
                                        
                                        </ul>
                                    </div>


                                    <!-- Patents -->
                                    <div class="col-md-10 col-md-offset-1" style="margin-top: 75px;">
                                        <div class="title text-center">
                                            <h3 style="font-weight: 400;">Patents</h3>
                                        </div>
                                        <ul class="ul-withdetails">
                                            
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="scenario-generation" src="images/research/pubs/PatentScheduler.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Method and system for radio-resource scheduling in telecommunication-network                                                   
                                                            </h3>
                                                            <p> Jitender Singh Shekhawat, <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Anshuman Nigam, Konchady Gautam Shenoy, Yash Jain </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                US Patent 2022 &nbsp;
                                                                [<a href="https://patents.google.com/patent/US11523411B2/en" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details" style="padding-bottom: 2%;">
                                                    <p>
                                                        <strong>Abstract</strong>: The present disclosure provides a method for radio-resource scheduling in a telecommunication network. The method comprises selecting at least one objective associated with a radio-resource scheduling from a plurality of objectives; prioritizing at least one flow from a plurality of flows for the selected at least one objective; identifying at least one state parameter from a plurality of state parameters associated with at least one of an active bearers from a plurality of active bearers; inputting at least one of the plurality of state parameters for the at least one of the active bearers to be scheduled during a current transmission time interval (TTI) to a reinforcement machine learning (ML) network, the reinforcement ML network being configured for a reward in accordance with the selected at least one objective; and receiving, from the reinforcement ML network, a radio resource allocation for each of the active bearers for the current TTI.
                                                    </p>

                                                </div>
                                            </li>
                                        </ul>
                                    </div>


                                    <!-- Projects -->
                                    <div class="col-md-10 col-md-offset-1" style="margin-top: 75px;">
                                        <div class="title text-center">
                                            <h3 style="font-weight: 400;">Projects</h3>
                                        </div>
                                        <ul class="ul-withdetails">
                                            
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="video thumb">
                                                            <video class="responsive-video" controls>
                                                                <source src="videos/FrankaKitchen.mp4" type="video/mp4">
                                                                Your browser does not support the video tag.
                                                            </video>
                                                            <div class="videooverlay">
                                                                <i class="fa fa-play-circle"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Action-Quantized Offline Reinforcement Learning</h3>
                                                            <p> Leveraged VQ-VAE (Vector Quantised-Variational AutoEncoder) for state-conditioned action quantization (SAQ),
addressing the challenges of approximation in continuous action settings, which typically lead to performance
degradation. Extended this approach to enable joint end-to-end learning of quantization and policy, resulting in
approximately 20% performance improvements on locomotion, adroit, and kitchen tasks.
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details" style="padding-bottom: 2%;">
                                                    <p>
                                                        <strong>Abstract</strong>: The field of offline reinforcement learning (RL) offers a versatile framework for transforming fixed behavior datasets into policies that have the potential to surpass the performance of the original data-collecting policy. Despite significant advancements like adding conservatism and policy constraints to address distributional shifts, continuous action settings often pose challenges that necessitate approximations. In contrast, discrete action settings offer more precise or even exact computations for offline RL constraints and regularizers, presenting fewer hurdles. Our project begins with an exploration of an adaptive method for action quantization. Utilizing a VQ-VAE, we acquire knowledge in state-conditioned action quantization to address the exponential complexity inherent in naive action space discretization. Through experimentation, we reproduce that integrating this discretization technique strengthens the effectiveness of well-known offline RL approaches such as IQL and CQL on standardized tasks. We subsequently refine this methodology through joint training of VQ-VAE and offline RL methods, resulting in further performance enhancements compared to previous methodology.
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/SAQ_Alternating.png" 
                                                             class="img-responsive" style="scale: 0.8; display: block; margin: 0 auto;">
                                                    </p>
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/SAQ_Joint.png" 
                                                             class="img-responsive" style="scale: 0.8; display: block; margin: 0 auto;">
                                                    </p>


                                                </div>
                                            </li>

                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb" style="padding:0px;">
                                                            <img alt="image" src="images/research/inverse-kinematics-teaser.png" 
                                                            style="height: inherit; border-radius: inherit;" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Inverse Kinematic Algorithms for Spatially Hyper Redundant Bodies</h3>
                                                            <p> Metaheuristic optimization, inverse kinematics, Closed-loop control algorithm design. </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Spatially hyper redundant systems have more number of controllable Degrees of Freedom (DOF) as compared 
                                                        to their actual DOF. These systems have infinite number of solutions for a given state space reach making it complex to develop proper 
                                                        inverse kinematic solution. Adapting the optimization methods only help to arrive at the promising Inverse Kinematic (IK) solution. 
                                                        The second part of the project involves implementation and simulation of computed torque control method for a 2-DOF manipulator 
                                                        sing MATLAB/Simulink. Computed Torque Control is a powerful non-linear controller which uses feedback linearisation to compute 
                                                        the required arm torques required for movement. The robot model is designed using the SimMechanics library of Simulink.
                                                    </p>
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/inverse-kinematics-1.png" 
                                                             class="img-responsive" style="display: block; margin: 20px auto; width: 40%; ">
                                                    </p>
                                                </div>
                                            </li>

                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb" style="padding:0px;">
                                                            <img alt="image" src="images/research/prosthetic-knee-teaser.png" 
                                                            style="height: inherit; border-radius: inherit;" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Galerkin Finite Element Analysis of Below-knee Prosthesis</h3>
                                                            <p> Crank-Nicolson scheme, weak Galerkin, Stress analysis. </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: This study aims to identify the best possible material for production of liners for prosthetic limbs. 
                                                        Based on the standard Galerkin finite element method in space and Crank-Nicolson difference method in time, the semi-discrete 
                                                        and fully discrete systems are constructed. The code is written in C++ and MATLAB, and deformation plots of different loading 
                                                        conditions for different materials are analyzed. The code is a general approach written for a (n x m) meshing domain and can be 
                                                        refined as per the user preference based on the desired accuracy. The code was validated with simulations on ANSYS Static 
                                                        Structural providing a green signal for further research. Further work to incorporate the nonlinear constitutive behavior of 
                                                        silicone will be done to test whether silicone is really the best economic material in the market available.
                                                    </p>
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/prosthetic-knee-1.png" 
                                                             class="img-responsive" style="display: block; margin: 20px auto; width: 30%; ">
                                                    </p>
                                                </div>
                                            </li>
                                        
                                        </ul>
                                    </div>


                                    

                                </div>
                            </div>
                        </div>    
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>

