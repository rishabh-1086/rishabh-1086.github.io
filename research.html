<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Rishabh Agrawal</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="owwwlab.com">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        
        <meta name="description" content="Rishabh Agrawal USC Samsung IIT Guwahati UMN Hanyang" />
        <meta name="keywords" content="Rishabh Agrawal, USC, Samsung, IIT Guwahati, UMN, Hanyang" />

        <!--<link rel="shortcut icon" href="./images/ra-icon.png">-->
        <link rel="shortcut icon" href="./images/aa-icon.png" type="image/png">


        <!--CSS styles-->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">  
        <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
        <link rel="stylesheet" href="css/magnific-popup.css">
        <link rel="stylesheet" href="css/style.css">
        <link id="theme-style" rel="stylesheet" href="css/styles/blue.css">

        
        <!--/CSS styles-->
        <!--Javascript files-->
        <script type="text/javascript" src="js/jquery-1.11.3.min.js"></script>
        <script type="text/javascript" src="js/TweenMax.min.js"></script>
        <script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
        <script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>
        
        <script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
        <script type="text/javascript" src="js/jquery.dropdownit.js"></script>

        <script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

        <script type="text/javascript" src="js/bootstrap.min.js"></script>

        <script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

        <script type="text/javascript" src="js/masonry.min.js"></script>

        <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
        <script type="text/javascript" src="js/jquery.nicescroll.min.js"></script>

        <script type="text/javascript" src="js/magnific-popup.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>

        <!--/Javascript files-->

    </head>
    <body>

        <div id="wrapper">
            <a href="#sidebar" class="mobilemenu"><i class="fa fa-reorder"></i></a>

            <div id="sidebar">
                <div id="sidebar-wrapper">
                    <div id="sidebar-inner">
                        <!-- Profile/logo section-->
                        <div id="profile" class="clearfix">
                            <a href="index.html">
                                <div class="portrait hidden-xs"></div>
                                <div class="title">
                                    <h3>Rishabh Agrawal</h3>
                                    <!-- <h3>University of Southern California</h3> -->
                            </a>
                            <a title="USC" href="https://www.usc.edu/" target="_blank">
                                <h5 style="color: #aaa">University of Southern California,</h5> 
                                <h5 style="margin-top: -3%; color: #aaa;"> Los Angeles</h5>
                            </a>
                        </div>   
                    </div>
                        <!-- /Profile/logo section-->

                        <!-- Main navigation-->
                        <div id="main-nav">
                            <ul id="navigation">
                                <li>
                                  <a href="index.html">
                                    <i class="fa fa-user"></i>
                                    <div class="text">About</div>
                                  </a>
                                </li>  
                                
                                <li class="currentmenu">
                                  <a href="research.html">
                                    <i class="fa fa-edit"></i>
                                    <div class="text">Research</div>
                                  </a>
                                </li> 


                                 <!-- <li>
                                  <a href="life.html">
                                    <i class='fa fa-heartbeat'></i>
                                      <div class="text">Life</div>
                                  </a>
                                </li>


                                <li>
                                  <a href="contact.html">
                                      <i class="fa fa-map-marker"></i>
                                      <div class="text">Find Me</div>
                                  </a>
                                </li> -->

                            </ul>
                        </div>


                        <!-- Social media -->

                        <div id="rcorners">
                            <div class="grid-container">
                                <item><a title="Email" href="mailto:rishabha@usc.edu"><i class="fa fa-envelope"></i></a></item>
                                <item><a title="GitHub" href="https://github.com/rishabh-1086" target="_blank"><i class="fa fa-github""></i></a></item>
                                <item><a title="Google Scholar" href="https://scholar.google.com/citations?user=LSIA40MAAAAJ&hl=en" target="_blank"><i class="fa fa-graduation-cap" style="transform: scale(-1, 1);"></i></a></item>
                                <item><a title="LinkedIn" href="https://www.linkedin.com/in/rishabh-694/" target="_blank"><i class="fa fa-linkedin"></i></a></item>
                                <item><a title="Facebook" href="https://www.facebook.com/rishabh.agrawal.7/" target="_blank"><i class="fa fa-facebook-official"></i></a></item>
                                <item><a title="Instagram" href=https://www.instagram.com/__rishabh.agrawal__/" target="_blank"><i class="fa fa-instagram"></i></a></item>
                                <item></item>
                                <!-- <item>
                                    <a title="Resume" href="https://drive.google.com/file/d/1wxloraTS4kUvf7WtTnpk8pEpm3jT1Eux/view?usp=sharing" target="_blank"><i class="fa fa-file"></i></a>                                  
                                </item> -->
                                <item>
                                    <a title="Resume" href="https://drive.google.com/file/d/1HXflkbFMkekkb3gF9U-okVL8nOGzdNnU/view?usp=sharing" target="_blank">
                                        <span class="fa-stack fa-1x">
                                            <i class="fa fa-file fa-stack-1x"></i>
                                              <span class="fa fa-stack-1x" style="color:rgb(0, 0, 0);">
                                                  <span style="font-size:11px; margin-top:0px; font-weight: bold;">
                                                      CV
                                                  </span>
                                            </span>
                                        </span>	
                                
                                    </a>
                                </item>
                                <item></item>                                    
                            </div>
                        </div>

                        <!-- /Main navigation-->
                        <!-- Sidebar footer -->
                        <div id="sidebar-footer">
                            <div class="lastupdate" style="padding: 5px;">Last Updated: January 2025</div>
                            <!-- <div class="social-icons">
                                <ul>
                                    <li><a href="https://github.com/agnihotriakhil"><i class="fa fa-github"></i></a></li>
                                    <li><a href="https://scholar.google.com/citations?user=Kf1o27gAAAAJ&hl=en"><i class="fa fa-google"></i></a></li>
                                    <li><a href="https://www.linkedin.com/in/akhil-agnihotri-549690137/"><i class="fa fa-linkedin"></i></a></li>
                                    <li><a href="https://www.facebook.com/akhil.agnihotri.35"><i class="fa fa-facebook"></i></a></li>
                                    <li><a href="https://www.instagram.com/longstory_short_/"><i class="fa fa-instagram"></i></a></li>
                                </ul>
                            </div> -->
                            <!-- <div id="copyright">
                                <i class="fa fa-copyright">Akhil Agnihotri</i>
                            </div> -->
                    
                        </div>
                        <!-- /Sidebar footer -->
                    </div>

                </div>
            </div>

            <div id="main">
            
                <div id="research" class="page">
                    <div class="pageheader">

                        <div class="headercontent">

                            <div class="section-container">
                                <h3 class="title" > Research Summary </h3>
                            
                                <div class="row" style="margin: 0px 0px 0px 10px;">
                                    <div class="col-md">
                                    <font size="-0.5">
                                        <p>
                                            I am passionate about addressing problems that have a direct and meaningful impact on our community. My work has been dedicated to understanding the nature of these challenges and exploring solutions rooted in machine learning, probability, optimization, and simulation.
                                        </p>
                                        <p> 
                                            <strong>Interests</strong>: Reinforcement Learning, Imitation Learning, Generative AI, Behavior Foundation Models, Large Language Models.
                                        </p>
                                    </font>
                                    </div>
                                    <!-- <div class="col-md-4">
                                        <div class="subtitle text-center">
                                            <h3>Interests</h3>
                                        </div>
                                        <ul class="ul-boxed list-unstyled">
                                            <li>Data Mining</li>
                                            <li>Machine Learning</li>
                                            <li>Deep Learning</li>
                                            <li>Recommendation Systems</li>
                                            <li>User Online Experience</li>
                                            <li> Social Media </li>
                                            <li> Big Data </li>
                                        </ul>
                                    </div> -->
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="pagecontents">
                        
                        <div class="section color-1">
                            <div class="section-container">
                                
                                <div class="row">

                                    <!-- Publications -->
                                    <div class="col-md-10 col-md-offset-1" >
                                        <div class="title text-center">
                                            <h3 style="font-weight: 400;">Publications</h3>
                                        </div>
                                        <ul class="ul-withdetails">



                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/bedroil.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 33px;">Balance Equation-based Distributionally Robust Offline Imitation Learning                                                     
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Yusuf Alvi, Rahul Jain, Ashutosh Nayyar.</p>
                                                            <p style="font-size:14.5px; margin-top: -5px;" >
                                                                Embodied and SafeAssured Robotic System, NeurIPS 2025 <b>(Oral)</b> &nbsp;
                                                                [<a href="https://openreview.net/pdf?id=rkUrkqEKZE" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Imitation Learning (IL) has proven highly effective for robotic and control tasks
where manually designing reward functions or explicit controllers is infeasible.
However, standard IL methods implicitly assume that the environment dynamics
remain fixed between training and deployment. In practice, this assumption rarely
holds where modeling inaccuracies, real-world parameter variations, and adversarial perturbations can all induce shifts in transition dynamics, leading to severe
performance degradation. We address this challenge through Balance Equationbased Distributionally Robust Offline Imitation Learning, a framework that learns
robust policies solely from expert demonstrations collected under nominal dynamics, without requiring further environment interaction. We formulate the problem as
a distributionally robust optimization over an uncertainty set of transition models,
seeking a policy that minimizes the imitation loss under the worst-case transition
distribution. Importantly, we show that this robust objective can be reformulated
entirely in terms of the nominal data distribution, enabling tractable offline learning.
Empirical evaluations on continuous-control benchmarks demonstrate that our approach achieves superior robustness and generalization compared to state-of-the-art
offline IL baselines, particularly under perturbed or shifted environments.
                                                    </p>
                                                    
                                                </div>
                                            </li>


                                            <!-- Markov Balance Satisfaction Improves Performance in Strictly Batch Offline Imitation Learning -->
                                            <li>
                                                <div class="row">
                                                    <!-- <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/preference_bandits_teaser.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div> -->
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="video thumb">
                                                            <video class="responsive-video" controls>
                                                                <source src="videos/HalfCheetah_video.mp4" type="video/mp4">
                                                                Your browser does not support the video tag.
                                                            </video>
                                                            <div class="videooverlay">
                                                                <i class="fa fa-play-circle"></i>
                                                            </div>
                                                        </div>
                                                    </div>


                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 32px;">Markov Balance Satisfaction Improves Performance in Strictly Batch Offline Imitation Learning                                                       
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar. </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                AAAI 2025 &nbsp;
                                                                [<a href="https://arxiv.org/pdf/2408.09125" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Imitation learning (IL) is notably effective for robotic tasks where directly programming behaviors or defining optimal control costs is challenging. In this work, we address a scenario where the imitator relies solely on observed behavior and cannot make environmental interactions during learning. It does not have additional supplementary datasets beyond the expert’s dataset nor any information about the transition dynamics. Unlike state-of-the-art (SOTA) IL methods, this approach tackles the limitations of conventional IL by operating in a more constrained and realistic setting. Our method uses the Markov balance equation and introduces a novel conditional density estimation-based imitation learning framework. It employs conditional normalizing flows for transition dynamics estimation and aims at satisfying a balance equation for the environment. Through a series of numerical experiments on Classic Control and MuJoCo environments, we demonstrate consistently superior empirical performance compared to many SOTA IL algorithms.
                                                    </p>
                                                    
                                                </div>
                                            </li>


                                            
                                            <!-- Conditional Kernel Imitation Learning for Continuous State Environments. -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="video thumb">
                                                            <video class="responsive-video" controls>
                                                                <source src="videos/LunarLander.mp4" type="video/mp4">
                                                                Your browser does not support the video tag.
                                                            </video>
                                                            <div class="videooverlay">
                                                                <i class="fa fa-play-circle"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 32px;">Conditional Kernel Imitation Learning for Continuous State Environments.                                                       
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar. </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                L4DC 2025 &nbsp;
                                                                [<a href="https://github.com/rishabh-1086/CKIL/blob/main/CKIL_Full_Paper.pdf" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Imitation Learning (IL) is an important paradigm within
the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of rewardfeedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods
such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is
particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems
which often require additional online interaction data to be
effective. In this paper, we consider the problem of imitation
learning in continuous state space environments based solely
on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any
additional interactions with the environment. Our approach
is based on the Markov balance equation and introduces a
novel conditional kernel density estimation-based imitation
learning framework. It involves estimating the environment’s
transition dynamics using conditional kernel density estimators and seeks to satisfy the probabilistic balance equations
for the environment. We establish that our estimators satisfy
basic asymptotic consistency requirements. Through a series
of numerical experiments on continuous state benchmark environments, we show consistently superior empirical performance over many state-of-the-art IL algorithms.
                                                    </p>
                                                    
                                                </div>
                                            </li>




                                            <!-- Policy Optimization for Strictly Batch Imitation Learning -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="video thumb">
                                                            <video class="responsive-video" controls>
                                                                <source src="videos/MountainCar.mp4" type="video/mp4">
                                                                Your browser does not support the video tag.
                                                            </video>
                                                            <div class="videooverlay">
                                                                <i class="fa fa-play-circle"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 32px;">Policy Optimization for Strictly Batch Imitation Learning                                                      
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar. </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                OPT for ML, NeurIPS 2024 &nbsp;
                                                                [<a href="https://opt-ml.org/papers/2024/paper84.pdf" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Imitation Learning (IL) offers a compelling framework within the broader context of Reinforcement Learning (RL) by eliminating the need for explicit reward feedback, a common requirement
in RL. In this work, we address IL based solely on observed behavior without access to transition
dynamics information, reward structure, or, most importantly, any additional interactions with the
environment. Our approach leverages conditional kernel density estimation and performs policy
optimization to ensure the satisfaction of the Markov balance equation associated with the environment. This method performs effectively in discrete and continuous state environments, providing
a novel solution to IL problems under strictly offline optimization settings. We establish that our
estimators satisfy basic asymptotic consistency requirements. Through a series of numerical experiments on continuous state benchmark environments, we show consistently superior empirical
performance over many state-of-the-art IL algorithms.
                                                    </p>
                                                    
                                                </div>
                                            </li>


                                            <!-- A Reinforcement Learning Framework for QoS-Driven Radio Resource Scheduler -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/RLScheduler.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3 style="margin-top: 33px;">A Reinforcement Learning Framework for QoS-Driven Radio Resource Scheduler                                                     
                                                            </h3>
                                                            <p> Jitender Singh Shekhawat, <strong style="font-weight: 600;">Rishabh Agrawal</strong>, K Gautam Shenoy, Rajath Shashidhara.</p>
                                                            <p style="font-size:14.5px; margin-top: -5px;" >
                                                                Globecom 2020 &nbsp;
                                                                [<a href="https://ieeexplore.ieee.org/document/9322182" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: In cellular communication systems, radio resources are allocated to users by the MAC scheduler, that typically runs at the base station (BS). The task of the scheduler is to meet the quality of service (QoS) requirements of each data flow while maximizing the system throughput and achieving a desired level of fairness amongst users. Traditional schedulers use handcrafted metrics and are meticulously tuned to achieve a delicate balance between multiple, often conflicting objectives. Diverse QoS requirements of 5G networks further complicate traditional schedulers. In this paper, we propose a novel reinforcement learning based scheduler that learns an allocation policy to simultaneously optimize multiple objectives. Our approach allows network operators to customize their requirements, by assigning priority values to QoS classes. In addition, we adopt a flexible neural-network architecture that can easily adapt to varying number of flows, drastically simplifying training, thus rendering it viable for practical implementation in constrained systems. We demonstrate, via simulations, that our algorithm outperforms conventional heuristics such as M-LWDF, EXP-RULE and LOGRULE and is robust to changes in radio environment and traffic patterns.
                                                    </p>
                                                    
                                                </div>
                                            </li>

                                            
                                            <!-- CoPASample: A Heuristics Based Covariance Preserving Data Augmentation -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/CopaSample.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>CoPASample: A Heuristics Based Covariance Preserving Data Augmentation                                                    
                                                            </h3>
                                                            <p> <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Paridhi Kothari. </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                LOD 2019 &nbsp;
                                                                [<a href="https://link.springer.com/chapter/10.1007/978-3-030-37599-7_26" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: An efficient data augmentation algorithm generates samples that improves accuracy and robustness of training models. Augmentation with informative samples imparts meaning to the augmented data set. In this paper, we propose CoPASample (Covariance Preserving Algorithm for generating Samples), a data augmentation algorithm that generates samples which reflects the first and second order statistical information of the data set, thereby augmenting the data set in a manner that preserves the total covariance of the data. To address the issue of exponential computations in the generation of points for augmentation, we formulate an optimisation problem motivated by the approach used in 
-SVR to iteratively compute a heuristics based optimal set of points for augmentation in polynomial time. Experimental results for several data sets and comparisons with other data augmentation algorithms validate the potential of our proposed algorithm.
                                                    </p>
                                                    
                                                </div>
                                            </li>

                                            <!-- Determining the Optimal Fuzzifier Range for Alpha-Planes of General Type-2 Fuzzy Sets -->
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="lidar-optimization" src="images/research/pubs/GT2.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-top: -10px; margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Determining the Optimal Fuzzifier Range for Alpha-Planes of General Type-2 Fuzzy Sets                                                       
                                                            </h3>
                                                            <p> Shreyas Kulkarni, <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Frank Chung-Hoon Rhee.</p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                FUZZ-IEEE 2018 &nbsp;
                                                                [<a href="https://ieeexplore.ieee.org/document/8491556" 
                                                                target="_blank">link</a>]
                        
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details">
                                                    <p>
                                                        <strong>Abstract</strong>: Type-2 fuzzy sets (T2 FSs) are capable of handling uncertainty more efficiently than type-1 fuzzy sets (T1 FSs). The fuzzifier parameter plays an important role in the final cluster partitions in fuzzy c-means (FCM), interval type-2 (IT2) FCM, general type-2 (GT2) FCM, and other fuzzy clustering algorithms. In general, fuzzifiers are chosen for a given dataset based on experience. In this paper, we adaptively compute suitable values for the range of the fuzzifier parameter for each α-plane of GT2 FSs for a given data set. The footprint of uncertainty (FOU) for each α-plane is obtained from the given data set using histogram based membership generation. This is iteratively processed to give the converged values of fuzzifier parameters for each α-plane of GT2 FSs. Experimental results for several data sets are given to validate the effectiveness of our proposed method.
                                                    </p>
                                                    
                                                </div>
                                            </li>

                                             
                                        
                                        </ul>
                                    </div>


                                    <!-- Patents -->
                                    <div class="col-md-10 col-md-offset-1" style="margin-top: 75px;">
                                        <div class="title text-center">
                                            <h3 style="font-weight: 400;">Patents</h3>
                                        </div>
                                        <ul class="ul-withdetails">
                                            
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb">
                                                            <img alt="scenario-generation" src="images/research/pubs/PatentScheduler.png" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Method and system for radio-resource scheduling in telecommunication-network                                                   
                                                            </h3>
                                                            <p> Jitender Singh Shekhawat, <strong style="font-weight: 600;">Rishabh Agrawal</strong>, Anshuman Nigam, Konchady Gautam Shenoy, Yash Jain </p>
                                                            <p style="font-size:14.5px; margin-top: -5px;">
                                                                US Patent 2022 &nbsp;
                                                                [<a href="https://patents.google.com/patent/US11523411B2/en" target="_blank">link</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details" style="padding-bottom: 2%;">
                                                    <p>
                                                        <strong>Abstract</strong>: The present disclosure provides a method for radio-resource scheduling in a telecommunication network. The method comprises selecting at least one objective associated with a radio-resource scheduling from a plurality of objectives; prioritizing at least one flow from a plurality of flows for the selected at least one objective; identifying at least one state parameter from a plurality of state parameters associated with at least one of an active bearers from a plurality of active bearers; inputting at least one of the plurality of state parameters for the at least one of the active bearers to be scheduled during a current transmission time interval (TTI) to a reinforcement machine learning (ML) network, the reinforcement ML network being configured for a reward in accordance with the selected at least one objective; and receiving, from the reinforcement ML network, a radio resource allocation for each of the active bearers for the current TTI.
                                                    </p>

                                                </div>
                                            </li>
                                        </ul>
                                    </div>


                                    <!-- Projects -->
                                    <div class="col-md-10 col-md-offset-1" style="margin-top: 75px;">
                                        <div class="title text-center">
                                            <h3 style="font-weight: 400;">Projects</h3>
                                        </div>
                                        <ul class="ul-withdetails">




                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb" style="padding:0px;">
                                                            <img alt="image" src="images/research/Diffusion.png" 
                                                            style="height: inherit; border-radius: inherit;" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Restoring Multimodal Missing Modalities with Diffusion Models
                                                            </h3>
                                                            <p> Proposed a methodology to utilized diffusion models for missing modality generation conditioned on available modalities. Specifically, conducted studies to answer these questions: a) How well diffusion models restore missing modalities? b) Whether reconstructed data improves personality trait predictions? c) How different missing modality scenarios (e.g., missing speech or vision) affect prediction performance? [<a href="https://github.com/rishabh-1086/Remodiff" target="_blank">code</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details" style="padding-bottom: 2%;">
                                                    <p>
                                                        <strong>Abstract</strong>: Multimodal learning has gained significant attention for its applications in critical areas such as computer vision, robotics, healthcare diagnostics, and human-computer interaction, where the ability to synthesize multiple modalities can significantly improve predictive accuracy and system robustness. However, real-world multimodal datasets are often incomplete, leading to degraded performance in predictive tasks. In this work, we propose a diffusion-based modality restoration framework that conditions a trained diffusion model on available modalities to reconstruct the missing one. We extract emotion-rich features from text, video and audio and explore various fusion strategies (early, late, and model-level transformer based fusion) to integrate reconstructed modalities for improved downstream personality trait inference. We evaluate our approach on the ChaLearn First Impressions V2 dataset.
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/ReMoDiff.png" 
                                                             class="img-responsive" style="scale: 0.8; display: block; margin: 0 auto;">
                                                    </p>
                                    


                                                </div>
                                            </li>



                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb" style="padding:0px;">
                                                            <img alt="image" src="images/research/Recommendation.png" 
                                                            style="height: inherit; border-radius: inherit;" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Reinforcement learning from Human Feedback (RLHF)-based Optimization of Recommendations
Using a Large Language Model (LLM)</h3>
                                                            <p> Proposed a recommendation system that integrates RLHF with an LLM to fine-tune recommendations and align the
reward model with user preferences. This approach aims to outperform both standalone LLM systems and traditional
algorithms like Collaborative Filtering by ensuring better alignment with user feedback. [<a href="https://github.com/rishabh-1086/LLM-RL4Rec" target="_blank">code</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details" style="padding-bottom: 2%;">
                                                    <p>
                                                        <strong>Abstract</strong>: This study explores the optimization of recommendation systems with Large Language Models (LLMs) using two distinct methodologies: Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO). Separate pipelines were developed to fine-tune LLMs for inferred user preferences. Using MovieLens datasets, we show that supervised fine-tuning (SFT) and preference tuning improve LLM-based recommendations. These results highlight the potential and limitations of LLM-based systems for recommendation tasks.                                                    </p>
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/RLHF.jpg" 
                                                             class="img-responsive" style="scale: 0.8; display: block; margin: 0 auto;">
                                                    </p>
                                    


                                                </div>
                                            </li>

                                            
                                            
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="video thumb">
                                                            <video class="responsive-video" controls>
                                                                <source src="videos/FrankaKitchen.mp4" type="video/mp4">
                                                                Your browser does not support the video tag.
                                                            </video>
                                                            <div class="videooverlay">
                                                                <i class="fa fa-play-circle"></i>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Action-Quantized Offline Reinforcement Learning</h3>
                                                            <p> Leveraged VQ-VAE (Vector Quantised-Variational AutoEncoder) for state-conditioned action quantization (SAQ),
addressing the challenges of approximation in continuous action settings, which typically lead to performance
degradation. Extended this approach to enable joint end-to-end learning of quantization and policy, resulting in
approximately 20% performance improvements on locomotion, adroit, and kitchen tasks. [<a href="https://github.com/rishabh-1086/saq-rl" target="_blank">code</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details" style="padding-bottom: 2%;">
                                                    <p>
                                                        <strong>Abstract</strong>: The field of offline reinforcement learning (RL) offers a versatile framework for transforming fixed behavior datasets into policies that have the potential to surpass the performance of the original data-collecting policy. Despite significant advancements like adding conservatism and policy constraints to address distributional shifts, continuous action settings often pose challenges that necessitate approximations. In contrast, discrete action settings offer more precise or even exact computations for offline RL constraints and regularizers, presenting fewer hurdles. Our project begins with an exploration of an adaptive method for action quantization. Utilizing a VQ-VAE, we acquire knowledge in state-conditioned action quantization to address the exponential complexity inherent in naive action space discretization. Through experimentation, we reproduce that integrating this discretization technique strengthens the effectiveness of well-known offline RL approaches such as IQL and CQL on standardized tasks. We subsequently refine this methodology through joint training of VQ-VAE and offline RL methods, resulting in further performance enhancements compared to previous methodology.
                                                    </p>
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/SAQ_Alternating.png" 
                                                             class="img-responsive" style="scale: 0.8; display: block; margin: 0 auto;">
                                                    </p>
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/SAQ_Joint.png" 
                                                             class="img-responsive" style="scale: 0.8; display: block; margin: 0 auto;">
                                                    </p>


                                                </div>
                                            </li>

                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb" style="padding:0px;">
                                                            <img alt="image" src="images/research/Earthquake.jpeg" 
                                                            style="height: inherit; border-radius: inherit;" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Earthquake Damage Prediction</h3>
                                                            <p> Predicted building damage levels using data from the 2015 Gorkha earthquake in Nepal. Employed feature engineering
and ensemble modeling with LightGBM, CatBoost, and XGBoost to develop a machine learning model, achieving an
F1−score of 0.7541 on test data and securing 2nd place out of 50 teams. [<a href="https://github.com/rishabh-1086/Earthquake-Damage-Prediction" target="_blank">code</a>]
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details" style="padding-bottom: 2%;">
                                                    <p>
                                                        <strong>Abstract</strong>: Predicting the damage caused by an earthquake is a challenging task. Given an extensive dataset comprised of aspects of building location and construction, we predict the damage level caused to buildings. The data is from the 2015 Gorkha earthquake in Nepal. We make use of feature engineering techniques, gradient boosting algorithms and ensemble models to develop our machine learning model, which achieved an F1 score of 0.7541 on test data.                                                    
                                                    </p>
                                                    <p>
                                                        <img alt="scenario-generation" src="images/research/MajorityVote.png" 
                                                             class="img-responsive" style="scale: 0.8; display: block; margin: 0 auto;">
                                                    </p>
                                    


                                                </div>
                                            </li>
                                            
                                            <li>
                                                <div class="row">
                                                    <div class="col-sm-4 col-md-3">
                                                        <div class="image thumb" style="padding:0px;">
                                                            <img alt="image" src="images/research/Data_Augmentation.jpg" 
                                                            style="height: inherit; border-radius: inherit;" class="img-responsive">
                                                            <div class="imageoverlay">
                                                                <i class="fa fa-search"></i>
                                                            </div>

                                                        </div>
                                                    </div>
                                                    <div class="col-sm-8 col-md-9" style="margin-left: -30px;">
                                                        <div class="meta">
                                                            <h3>Application of r-cyclic matrices in Data Augmentation</h3>
                                                            <p> Proposed a data augmentation algorithm to counter the problem of data insufficiency with the objective of preserving
covariance post augmentation. Provided a heuristics to convert the involved exponential time complexity into polynomial
time complexity. Formulated an optimization problem motivated by the approach used in ν-SVR. Experimented on
various UCI data sets using multiple classification algorithms to outline the effectiveness of the proposed work. 
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="details" style="padding-bottom: 2%;">
                                                    <p>
                                                        <strong>Abstract</strong>: An efficient data augmentation algorithm generates samples that improves accuracy and robustness of training models. Augmentation with informative samples imparts meaning to the augmented data set. In this paper, we propose CoPASample (Covariance Preserving Algorithm for generating Samples), a data augmentation algorithm that generates samples which reflects the first and second order statistical information of the data set, thereby augmenting the data set in a manner that pre- serves the total covariance of the data. To address the issue of exponential computations in the generation of points for augmentation, we formu- late an optimisation problem motivated by the approach used in ν-SVR to iteratively compute a heuristics based optimal set of points for aug- mentation in polynomial time. Experimental results for several data sets and comparisons with other data augmentation algorithms validate the potential of our proposed algorithm.
                                                    </p>

                                                </div>
                                            </li>
                                        
                                        </ul>
                                    </div>


                                    

                                </div>
                            </div>
                        </div>    
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>

